{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba425d-5dbc-4f97-9ff0-c502b203ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb pypdf openai langchain sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8cfea7-aa54-4261-9fd4-fe2f2333f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44d2a5a-c621-4ffc-8572-965c0e191c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "secret = os.getenv(\"OPENAI_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec9d723-7de1-434a-bd57-69bfbe54791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided by instructor \n",
    "def project_embeddings(embeddings, umap_transform):\n",
    "    \"\"\"\n",
    "    Projects the given embeddings using the provided UMAP transformer.\n",
    "\n",
    "    Args:\n",
    "    embeddings (numpy.ndarray): The embeddings to project.\n",
    "    umap_transform (umap.UMAP): The trained UMAP transformer.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The projected embeddings.\n",
    "    \"\"\"\n",
    "    projected_embeddings = umap_transform.transform(embeddings)\n",
    "    return projected_embeddings\n",
    "\n",
    "\n",
    "def word_wrap(text, width=87):\n",
    "    \"\"\"\n",
    "    Wraps the given text to the specified width.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to wrap.\n",
    "    width (int): The width to wrap the text to.\n",
    "\n",
    "    Returns:\n",
    "    str: The wrapped text.\n",
    "    \"\"\"\n",
    "    return \"\\n\".join([text[i : i + width] for i in range(0, len(text), width)])\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: The extracted text.\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        pdf = PdfReader(f)\n",
    "        for page_num in range(pdf.get_num_pages()):\n",
    "            page = pdf.get_page(page_num)\n",
    "            text.append(page.extract_text())\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "\n",
    "def load_chroma(filename, collection_name, embedding_function):\n",
    "    \"\"\"\n",
    "    Loads a document from a PDF, extracts text, generates embeddings, and stores it in a Chroma collection.\n",
    "\n",
    "    Args:\n",
    "    filename (str): The path to the PDF file.\n",
    "    collection_name (str): The name of the Chroma collection.\n",
    "    embedding_function (callable): A function to generate embeddings.\n",
    "\n",
    "    Returns:\n",
    "    chroma.Collection: The Chroma collection with the document embeddings.\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF\n",
    "    text = extract_text_from_pdf(filename)\n",
    "\n",
    "    # Split text into paragraphs or chunks\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "\n",
    "    # Generate embeddings for each chunk\n",
    "    embeddings = [embedding_function(paragraph) for paragraph in paragraphs]\n",
    "\n",
    "    # Create a DataFrame to store text and embeddings\n",
    "    data = {\"text\": paragraphs, \"embeddings\": embeddings}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create or load the Chroma collection\n",
    "\n",
    "    collection = chromadb.Client().create_collection(collection_name)\n",
    "\n",
    "    # Add the data to the Chroma collection\n",
    "    for ids, row in df.iterrows():\n",
    "\n",
    "        collection.add(ids=ids, documents=row[\"text\"], embeddings=row[\"embeddings\"])\n",
    "        # collection.add(text=row[\"text\"], embedding=row[\"embeddings\"])\n",
    "\n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d181b9-fd15-47ca-a9b9-47e3df839988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the text from the pdf \n",
    "reader = PdfReader(\"data/microsoft-annual-report.pdf\")\n",
    "#extracting the text from the pdf \n",
    "pdf_texts = [p.extract_text().strip() for p in reader.pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f918e754-e966-4fe6-8410-e1f57dc4f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the text into chunks using langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "#doing this helps it make more readable/structured\n",
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "                                                    #the seperators tell where to split\n",
    "                                                    #there is an order of prefernce\n",
    "                                                    #first try at \\n, then at \\n\\n, then at . \n",
    "                                                    separators = [\"\\n\", \"\\n\\n\", \". \", \" \", \"\"], \n",
    "                                                    chunk_size = 1000, \n",
    "                                                    chunk_overlap = 0)\n",
    "combo = \"\\n\\n\".join(pdf_texts)\n",
    "character_split_texts = character_splitter.split_text(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1417174-c6ad-417b-9424-76869ba9572c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import sentence_transformers python package. This is needed in order to for SentenceTransformersTokenTextSplitter. Please install it with `pip install sentence-transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_text_splitters\\sentence_transformers.py:22\u001b[0m, in \u001b[0;36mSentenceTransformersTokenTextSplitter.__init__\u001b[1;34m(self, chunk_overlap, model_name, tokens_per_chunk, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#now we want to split the text into chunks of about 256 tokens \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#making the object that will split the text by tokens \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m token_splitter \u001b[38;5;241m=\u001b[39m SentenceTransformersTokenTextSplitter(tokens_per_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m260\u001b[39m, chunk_overlap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#actually splitting by tokens \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#doing this for each textsplit \u001b[39;00m\n\u001b[0;32m      7\u001b[0m token_split_texts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_text_splitters\\sentence_transformers.py:24\u001b[0m, in \u001b[0;36mSentenceTransformersTokenTextSplitter.__init__\u001b[1;34m(self, chunk_overlap, model_name, tokens_per_chunk, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is needed in order to for SentenceTransformersTokenTextSplitter. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m     )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name)\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import sentence_transformers python package. This is needed in order to for SentenceTransformersTokenTextSplitter. Please install it with `pip install sentence-transformers`."
     ]
    }
   ],
   "source": [
    "#now we want to split the text into chunks of about 256 tokens \n",
    "#making the object that will split the text by tokens \n",
    "token_splitter = SentenceTransformersTokenTextSplitter(tokens_per_chunk = 260, chunk_overlap = 0)\n",
    "\n",
    "#actually splitting by tokens \n",
    "#doing this for each textsplit \n",
    "token_split_texts = []\n",
    "for text in character_split_texts:\n",
    "    token_split_texts += token_splitter.split_text(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4706151-c0df-4f13-9a83-c246bd6f224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#begin the embedding process \n",
    "import chromadb \n",
    "from chromadb.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "#instantiate the embedding function \n",
    "embedding_function = SentenceTransformerEmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86cdf512-c294-4899-9e3e-c355a929c52f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chromadb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#instantiate the chroma client \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m chroma_client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mclient()\n\u001b[0;32m      3\u001b[0m chroma_collection \u001b[38;5;241m=\u001b[39m chroma_client\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#name the function\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft collection\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#do the embedding \u001b[39;00m\n\u001b[0;32m      7\u001b[0m     embedding_function \u001b[38;5;241m=\u001b[39m embedding_function)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#extract the embeddings \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chromadb' is not defined"
     ]
    }
   ],
   "source": [
    "#instantiate the chroma client \n",
    "chroma_client = chromadb.client()\n",
    "chroma_collection = chroma_client.create_collection(\n",
    "    #name the function\n",
    "    \"microsoft collection\",\n",
    "    #do the embedding \n",
    "    embedding_function = embedding_function)\n",
    "#extract the embeddings \n",
    "ids = [str(i) for i in range(len(token_split_texts))]\n",
    "#now let us add the embeddings to the database \n",
    "chroma_collection.add(id = ids, documents = token_split_texts) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1575df8f-2c71-492d-a99e-300d3dba3eee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
