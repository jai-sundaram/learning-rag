{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7ec84f1-d45a-4490-a434-4525b7a0261a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\jesun\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.3.19-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.63 (from langchain_openai)\n",
      "  Downloading langchain_core-0.3.63-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting openai<2.0.0,>=1.68.2 (from langchain_openai)\n",
      "  Downloading openai-1.83.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from langchain_openai) (0.9.0)\n",
      "Collecting langsmith<0.4,>=0.1.126 (from langchain-core<1.0.0,>=0.3.63->langchain_openai)\n",
      "  Downloading langsmith-0.3.44-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.63->langchain_openai) (8.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.63->langchain_openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.63->langchain_openai) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.63->langchain_openai) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.63->langchain_openai) (4.11.0)\n",
      "Collecting pydantic>=2.7.4 (from langchain-core<1.0.0,>=0.3.63->langchain_openai)\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "     ---------------------------------------- 0.0/67.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.2/67.2 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.68.2->langchain_openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.66.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain_openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.63->langchain_openai) (2.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain_openai)\n",
      "  Downloading orjson-3.10.18-cp312-cp312-win_amd64.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.0/43.0 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain_openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.63->langchain_openai) (0.6.0)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.63->langchain_openai)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-extensions>=4.7 (from langchain-core<1.0.0,>=0.3.63->langchain_openai)\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.63->langchain_openai)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jesun\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.68.2->langchain_openai) (0.4.6)\n",
      "Downloading langchain_openai-0.3.19-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.5/64.5 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.63-py3-none-any.whl (438 kB)\n",
      "   ---------------------------------------- 0.0/438.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 438.5/438.5 kB 13.8 MB/s eta 0:00:00\n",
      "Downloading openai-1.83.0-py3-none-any.whl (723 kB)\n",
      "   ---------------------------------------- 0.0/723.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 723.4/723.4 kB 23.0 MB/s eta 0:00:00\n",
      "Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl (206 kB)\n",
      "   ---------------------------------------- 0.0/206.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 206.2/206.2 kB ? eta 0:00:00\n",
      "Downloading langsmith-0.3.44-py3-none-any.whl (361 kB)\n",
      "   ---------------------------------------- 0.0/362.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 362.0/362.0 kB 23.5 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "   ---------------------------------------- 0.0/444.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 444.2/444.2 kB ? eta 0:00:00\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.8/2.0 MB 57.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 41.4 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "   ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 43.8/43.8 kB ? eta 0:00:00\n",
      "Downloading orjson-3.10.18-cp312-cp312-win_amd64.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.8/134.8 kB 7.8 MB/s eta 0:00:00\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-extensions, orjson, jiter, typing-inspection, pydantic-core, pydantic, openai, langsmith, langchain-core, langchain_openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.14.6\n",
      "    Uninstalling pydantic_core-2.14.6:\n",
      "      Successfully uninstalled pydantic_core-2.14.6\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.5.3\n",
      "    Uninstalling pydantic-2.5.3:\n",
      "      Successfully uninstalled pydantic-2.5.3\n",
      "Successfully installed jiter-0.10.0 langchain-core-0.3.63 langchain_openai-0.3.19 langsmith-0.3.44 openai-1.83.0 orjson-3.10.18 pydantic-2.11.5 pydantic-core-2.33.2 typing-extensions-4.14.0 typing-inspection-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dcd99c8-7f1a-41a2-a0ca-fa5d45213e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\jesun\\anaconda3\\lib\\site-packages (0.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "043616b2-43db-4a76-ad64-3142f5db024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the documents \n",
    "question = \"What kind of pets do I like?\"\n",
    "document = \"My favorite pet is a dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "781b35f9-0318-4292-8534-4e5959186aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returning number of tokens in a text string \n",
    "import tiktoken \n",
    "def num_tokens_from_string(string: str, encoding_name: str):\n",
    "    #getting the encoder/tokenizer object\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    #encoding the string into to tokens, and then figuring out how many tokens there are \n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens \n",
    "#getting the number of tokens in the question \n",
    "num_tokens_from_string(question, \"cl100k_base\")\n",
    "#can understand how large the documents we want to feed in are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c73a685-3876-477e-b696-1ea14a553cf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#getting a fixed length vector representation for both the document and the query/question \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m----> 3\u001b[0m embd \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[0;32m      4\u001b[0m query_result \u001b[38;5;241m=\u001b[39m embd\u001b[38;5;241m.\u001b[39membed_query(question)\n\u001b[0;32m      5\u001b[0m query_document \u001b[38;5;241m=\u001b[39m embd\u001b[38;5;241m.\u001b[39membed_query(document)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:327\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.validate_environment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy)\n\u001b[0;32m    326\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msync_specific)\u001b[38;5;241m.\u001b[39membeddings  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_async_client:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_client.py:126\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    124\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m     )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "#getting a fixed length vector representation for both the document and the query/question \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "query_result = embd.embed_query(question)\n",
    "query_document = embd.embed_query(document)\n",
    "#getting the length of the vector representation for the query\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e185627-5361-42df-b601-f3c8afd33a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
